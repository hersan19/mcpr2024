{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d21e3d",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92919f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "'''Hyperparameters configuration'''\n",
    "#Convolutional layers\n",
    "FSIZES = [1,2,3,4,5,6,7]\n",
    "NFILTERS = [2,4,8,16,32]\n",
    "\n",
    "#Pooling layers\n",
    "PSIZES = [2,3,4,5]\n",
    "PTYPE = ['max', 'avg']\n",
    "\n",
    "#Fully connected layers\n",
    "NEURONS = [4,8,16,32,64,128]\n",
    "random.seed(5)\n",
    "\n",
    "class Encoding:\n",
    "    def __init__(self, minC, maxC, minF, maxF):\n",
    "\n",
    "        self.n_conv = random.randint(minC, maxC)\n",
    "        self.n_full = random.randint(minF, maxF)\n",
    "\n",
    "        '''First level encoding'''\n",
    "        self.first_level = []\n",
    "        #Feature extraction part\n",
    "        for i in range(self.n_conv):\n",
    "            layer = {'type' : 'conv',\n",
    "                     'nfilters' : random.choice(NFILTERS),\n",
    "                     'fsize' : random.choice(FSIZES),\n",
    "                     'pool' : random.choice(['max', 'avg', 'off']),\n",
    "                     'psize' : random.choice(PSIZES)\n",
    "                    }\n",
    "            self.first_level.append(layer)\n",
    "\n",
    "        #Fully connected part\n",
    "        for i in range(self.n_full):\n",
    "            layer = {'type' : 'fc',\n",
    "                     'neurons' : random.choice(NEURONS)}\n",
    "            self.first_level.append(layer)\n",
    "\n",
    "        '''Second level encoding'''\n",
    "        self.second_level = []\n",
    "        prev = -1\n",
    "        for i in range(self.n_conv):\n",
    "            if prev < 1:\n",
    "                prev += 1\n",
    "            if prev >= 1:\n",
    "                for _ in range(prev-1):\n",
    "                    self.second_level.append(random.choice([0,1]))\n",
    "                prev += 1\n",
    "\n",
    "e = Encoding(2,8,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f55a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: torchtext in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Collecting pytorch-forecasting\n",
      "  Obtaining dependency information for pytorch-forecasting from https://files.pythonhosted.org/packages/43/80/79c0bb9f064c87cd282029f4db7bbdde333c91824c28b59739622770f542/pytorch_forecasting-0.10.1-py3-none-any.whl.metadata\n",
      "  Using cached pytorch_forecasting-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from pytorch-forecasting) (3.7.2)\n",
      "Collecting optuna<3.0.0,>=2.3.0 (from pytorch-forecasting)\n",
      "  Obtaining dependency information for optuna<3.0.0,>=2.3.0 from https://files.pythonhosted.org/packages/9f/6c/c6fab7d673d9d12144b8fe7155e15aeb06798b029ecfece4b193ffc859e6/optuna-2.10.1-py3-none-any.whl.metadata\n",
      "  Using cached optuna-2.10.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pandas<2.0.0,>=1.3.0 (from pytorch-forecasting)\n",
      "  Obtaining dependency information for pandas<2.0.0,>=1.3.0 from https://files.pythonhosted.org/packages/da/6d/1235da14daddaa6e47f74ba0c255358f0ce7a6ee05da8bf8eb49161aa6b5/pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting pytorch-lightning<2.0.0,>=1.2.4 (from pytorch-forecasting)\n",
      "  Obtaining dependency information for pytorch-lightning<2.0.0,>=1.2.4 from https://files.pythonhosted.org/packages/77/ed/7d91e1958f0d48b439fae0de8ece3de3ce8c3d4e03b04bd3c007ba879a49/pytorch_lightning-1.9.5-py3-none-any.whl.metadata\n",
      "  Using cached pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting scikit-learn<1.1,>=0.24 (from pytorch-forecasting)\n",
      "  Using cached scikit-learn-1.0.2.tar.gz (6.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [66 lines of output]\n",
      "  Partial import of sklearn during the build process.\n",
      "  setup.py:128: DeprecationWarning:\n",
      "  \n",
      "    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "    of the deprecation of `distutils` itself. It will be removed for\n",
      "    Python >= 3.12. For older Python versions it will remain present.\n",
      "    It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "    For more details, see:\n",
      "      https://numpy.org/devdocs/reference/distutils_status_migration.html\n",
      "  \n",
      "  \n",
      "    from numpy.distutils.command.build_ext import build_ext  # noqa\n",
      "  INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\David Velazco\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\David Velazco\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\anaconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "      return hook(metadata_directory, config_settings)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 174, in prepare_metadata_for_build_wheel\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 268, in run_setup\n",
      "      self).run_setup(setup_script=setup_script)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 158, in run_setup\n",
      "      exec(compile(code, __file__, 'exec'), locals())\n",
      "    File \"setup.py\", line 319, in <module>\n",
      "      setup_package()\n",
      "    File \"setup.py\", line 315, in setup_package\n",
      "      setup(**metadata)\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\numpy\\distutils\\core.py\", line 135, in setup\n",
      "      config = configuration()\n",
      "               ^^^^^^^^^^^^^^^\n",
      "    File \"setup.py\", line 201, in configuration\n",
      "      config.add_subpackage(\"sklearn\")\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1050, in add_subpackage\n",
      "      config_list = self.get_subpackage(subpackage_name, subpackage_path,\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1016, in get_subpackage\n",
      "      config = self._get_configuration_from_setup_py(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 958, in _get_configuration_from_setup_py\n",
      "      config = setup_module.configuration(*args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-install-d2yc8zuh\\scikit-learn_4dac36692a2f46f5938cfdcb688ae199\\sklearn\\setup.py\", line 85, in configuration\n",
      "      cythonize_extensions(top_path, config)\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-install-d2yc8zuh\\scikit-learn_4dac36692a2f46f5938cfdcb688ae199\\sklearn\\_build_utils\\__init__.py\", line 47, in cythonize_extensions\n",
      "      basic_check_build()\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-install-d2yc8zuh\\scikit-learn_4dac36692a2f46f5938cfdcb688ae199\\sklearn\\_build_utils\\pre_build_helpers.py\", line 114, in basic_check_build\n",
      "      compile_test_program(code)\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-install-d2yc8zuh\\scikit-learn_4dac36692a2f46f5938cfdcb688ae199\\sklearn\\_build_utils\\pre_build_helpers.py\", line 70, in compile_test_program\n",
      "      ccompiler.compile(\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 327, in compile\n",
      "      self.initialize()\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\_msvccompiler.py\", line 224, in initialize\n",
      "      vc_env = _get_vc_env(plat_spec)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\msvc.py\", line 316, in msvc14_get_vc_env\n",
      "      return _msvc14_get_vc_env(plat_spec)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\David Velazco\\AppData\\Local\\Temp\\pip-build-env-rnllbljq\\overlay\\Lib\\site-packages\\setuptools\\msvc.py\", line 270, in _msvc14_get_vc_env\n",
      "      raise distutils.errors.DistutilsPlatformError(\n",
      "  distutils.errors.DistutilsPlatformError: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: xlrd in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: xlwt in c:\\users\\david velazco\\anaconda3\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio torchtext\n",
    "!pip install pytorch-forecasting\n",
    "!pip install pandas\n",
    "!pip install torchsummary\n",
    "!pip install openpyxl\n",
    "!pip install xlrd xlwt\n",
    "#!pip uninstall torchvision\n",
    "#!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf587e3c",
   "metadata": {},
   "source": [
    "## Librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da6f8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41257e",
   "metadata": {},
   "source": [
    "## Códigos adjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a90e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from DataLoaderRGB import *\n",
    "#from Training import *\n",
    "from encoding import *\n",
    "from decoding import *\n",
    "from operators import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eb8cd",
   "metadata": {},
   "source": [
    "## Sincrinizamos la GPU donde corramos el código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91651141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa22322",
   "metadata": {},
   "source": [
    "## Preparamos el data set, y lo convertimos en tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92a3f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labels = []  # Lista de etiquetas de clase\n",
    "        labels_file = os.path.join(root_dir, \"data.txt\")\n",
    "        with open(labels_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                label_degrees = float(parts[1])  # Etiqueta de clase en grados\n",
    "                label_radians = np.deg2rad(label_degrees)  # Convertir a radianes\n",
    "                self.labels.append(label_radians)\n",
    "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name)  # Abrir imagen en su formato original (RGB)\n",
    "\n",
    "        # Redimensionar la imagen a 256x256\n",
    "        #image = image.resize((256, 256), Image.BILINEAR)\n",
    "        image = image.resize((256, 256), Image.Resampling.BILINEAR)\n",
    "\n",
    "        image = np.array(image)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        sample = {\"image\": image, \"label\": label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[\"image\"], sample[\"label\"]\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {\"image\": torch.from_numpy(image), \"label\": label}\n",
    "\n",
    "def loading_data():\n",
    "    \n",
    "    root_dir = r'C:\\Users\\David Velazco\\Desktop\\MIA UV 2022-2024\\Tesis\\Pruebas\\CNN-Steering Angle\\dataset_self_driving_100_images'\n",
    "    custom_dataset = CustomDataset(root_dir=root_dir, transform=transforms.Compose([ToTensor()]))\n",
    "    train_size = int(0.7 * len(custom_dataset))\n",
    "    test_size = len(custom_dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [train_size, test_size])\n",
    "    print('Data size training: ', len(train_dataset))\n",
    "    print('Data size test: ', len(test_dataset))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7d1e0",
   "metadata": {},
   "source": [
    "## Funciones de entrenamiento de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd96bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "def entrenamiento(device, cnn, max_epochs, loss_func, optimizer, train_dl):\n",
    "    cnn.train()  # Configurando la red en modo de entrenamiento\n",
    "    torch.set_default_dtype(torch.float32)  # Establecer el tipo de tensor por defecto\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        # Iterando sobre los batches\n",
    "        for data in train_dl:\n",
    "            # Leyendo datos del batch\n",
    "            xb, yb = data['image'].to(device, dtype=torch.float32), data['label'].to(device, dtype=torch.float32)\n",
    "\n",
    "            # Obteniendo salida de la CNN\n",
    "            y_pred = cnn(xb)\n",
    "            yb = yb.unsqueeze(1)\n",
    "\n",
    "            # Obteniendo valor de la función de costo (MSE)\n",
    "            loss = loss_func(y_pred, yb)\n",
    "\n",
    "            # Obteniendo gradiente y hacer paso hacia atrás\n",
    "            loss.backward()  # Paso hacia atrás (derivadas)\n",
    "            optimizer.step()  # Ajustando pasos con optimizador\n",
    "            optimizer.zero_grad(set_to_none=True)  # Reiniciando gradientes a 0\n",
    "\n",
    "    return loss.item(), cnn  # Devuelve la pérdida final y la CNN entrenada\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def validacion(device, cnn, test_dl):\n",
    "    '''Inicializar perdida (Linea 1)'''\n",
    "    cnn.eval()  # Configurando la red en modo evaluación\n",
    "\n",
    "    '''Iterando sobre los batches (si el tamaño de batch es de 100, solo habrá un batch) (Linea 2)'''\n",
    "    for i, data in enumerate(test_dl, 0):\n",
    "        '''Leyendo datos del batch (Linea 3 y Linea 4)'''\n",
    "        xb, yb = data['image'], data['label']  # Leyendo entrada y etiqueta\n",
    "        xb = xb.type(torch.DoubleTensor).to(device, dtype=torch.float32)  # Moviendo entrada a GPU\n",
    "        yb = yb.to(device, dtype=torch.float64)  # Moviendo etiqueta a GPU\n",
    "\n",
    "        '''Obteniendo salida de la CNN (Linea 5)'''\n",
    "        y_pred = cnn(xb).detach().cpu().numpy()\n",
    "        y = yb.detach().cpu().numpy()\n",
    "\n",
    "    # Calculando el coeficiente de determinación R^2\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    MAPE = np.mean(np.abs((y) - (y_pred + 1)/(y)))\n",
    "    SMAPE = np.mean(np.abs((y_pred) - (y))/(np.abs(y) + np.abs(y_pred)))\n",
    "    return r2, MAPE, SMAPE\n",
    "\n",
    "\n",
    "\n",
    "def training(device, model, n_epochs, loss_func, train_dl, test_dl, lr, w, max_params):\n",
    "\n",
    "    #Number of parameters\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Move CNN to GPU\n",
    "    model.to(device)\n",
    "\n",
    "    #Optimizer\n",
    "    opt = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    #Obtaining mse\n",
    "    mse, cnn = entrenamiento(device, model, n_epochs, loss_func, opt, train_dl)\n",
    "\n",
    "    R2, MAPE,SMAPE = validacion(device, cnn, test_dl)\n",
    "    \n",
    "    #Append results to multiprocessing list\n",
    "    return MAPE,R2, params, mse,f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851543d",
   "metadata": {},
   "source": [
    "## Configuración de parametros del algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d4de72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximun and minimum numbers of layers to initialize networks\n",
    "\n",
    "min_conv = 2\n",
    "max_conv = 6\n",
    "min_full = 1\n",
    "max_full = 4\n",
    "\n",
    "'''Genetic Algorithm Parameters'''\n",
    "cr = 0.8 #Crossover rate\n",
    "mr = 0.5 #Mutation rate\n",
    "N = 10 #Population size\n",
    "T = 5 #Number of generations\n",
    "t_size = 5 #tournament size\n",
    "w = 0.3 #penalization weight\n",
    "max_params = 4e6\n",
    "num_epochs =  10\n",
    "lr = 1e-3\n",
    "\n",
    "#Defining loss function\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f143a59",
   "metadata": {},
   "source": [
    "### Instancias de datos en entrenamiento para la CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c95ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size training:  70\n",
      "Data size test:  30\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = loading_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7398e9",
   "metadata": {},
   "source": [
    "## Verificamos la dimensionalidad de los tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "379660a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 256, 256])\n",
      "tensor([ 0.0370, -0.0141, -0.0124,  0.0387,  0.0000,  0.0000,  0.0298,  0.1232,\n",
      "         0.0000,  0.0264], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Dimensiones del tensor test\"\"\"\n",
    "for i_batch,sample_batched in enumerate(test_dl):\n",
    "  print(sample_batched['image'].shape)\n",
    "  print(sample_batched['label'])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e873fd",
   "metadata": {},
   "source": [
    "## Creación de la población en el algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13c41456",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize population\n",
      "Paramameters:30500801, MAPE:0.05393765475581225\n",
      "Paramameters:430741, MAPE:0.2585938966116701\n",
      "Paramameters:131693, MAPE:0.05668148641615921\n",
      "Paramameters:1969753, MAPE:0.1775802787587987\n",
      "Paramameters:65343, MAPE:0.04032496979453134\n",
      "Paramameters:7009577, MAPE:0.25868186995944725\n",
      "Paramameters:33105, MAPE:0.028364491778065104\n",
      "Paramameters:3723529, MAPE:0.06367921290730327\n",
      "Paramameters:330777, MAPE:0.0942950341162987\n",
      "Paramameters:106289, MAPE:0.015300288238282023\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "'''Initialize population'''\n",
    "print('Initialize population')\n",
    "start = timeit.default_timer()\n",
    "pop = []\n",
    "mape = []\n",
    "R = []\n",
    "bestFitness = []\n",
    "bestParams = []\n",
    "bestMSE = []\n",
    "\n",
    "while len(pop) < N:\n",
    "\n",
    "    #Creating genomes (genetic encoding)\n",
    "    e1 = Encoding(min_conv,max_conv,min_full,max_full)\n",
    "\n",
    "    #Decoding the networks\n",
    "    network1 = decoding(e1)\n",
    "\n",
    "    #Creating the CNNs\n",
    "    cnn1 = CNN(e1, network1[0], network1[1], network1[2])\n",
    "    MAPE,R2, params, mse,f=  training(device, cnn1, num_epochs, loss_func, train_dl, test_dl, lr, w, max_params)\n",
    "    print(\"Paramameters:{}, MAPE:{}\".format(params, MAPE))\n",
    "    pop.append([e1,MAPE, params, mse])\n",
    "print('Finish')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0aa8d",
   "metadata": {},
   "source": [
    "## Algoritomo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "620d24e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  0\n",
      "Best MSE:  0.00013553407916333526\n",
      "Best MAPE:  0.015300288238282023\n",
      "Best No. of Params:  106289\n",
      "No. of Conv. Layers:  4\n",
      "No. of FC Layers:  3\n",
      "--------------------------------------------\n",
      "Generation:  1\n",
      "Best MSE:  0.00013553407916333526\n",
      "Best MAPE:  0.015300288238282023\n",
      "Best No. of Params:  106289\n",
      "No. of Conv. Layers:  4\n",
      "No. of FC Layers:  3\n",
      "--------------------------------------------\n",
      "Generation:  2\n",
      "Best MSE:  0.00013553407916333526\n",
      "Best MAPE:  0.015300288238282023\n",
      "Best No. of Params:  106289\n",
      "No. of Conv. Layers:  4\n",
      "No. of FC Layers:  3\n",
      "--------------------------------------------\n",
      "Generation:  3\n",
      "Best MSE:  0.00013553407916333526\n",
      "Best MAPE:  0.015300288238282023\n",
      "Best No. of Params:  106289\n",
      "No. of Conv. Layers:  4\n",
      "No. of FC Layers:  3\n",
      "--------------------------------------------\n",
      "Generation:  4\n",
      "Best MSE:  0.00013553407916333526\n",
      "Best MAPE:  0.015300288238282023\n",
      "Best No. of Params:  106289\n",
      "No. of Conv. Layers:  4\n",
      "No. of FC Layers:  3\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''Genetic Algorithm'''\n",
    "\n",
    "for t in range(T):\n",
    "    print('Generation: ', t)\n",
    "\n",
    "    #Parents Selection\n",
    "    parents = []\n",
    "    while len(parents) < int(N/2):\n",
    "        #Tournament Selection\n",
    "        tournament = random.sample(pop, t_size)\n",
    "        p1 = selection(tournament, 'min')\n",
    "        tournament = random.sample(pop, t_size)\n",
    "        p2 = selection(tournament, 'min')\n",
    "\n",
    "        while p1 == p2:\n",
    "            tournament = random.sample(pop, t_size)\n",
    "            p2 = selection(tournament, 'min')\n",
    "\n",
    "        parents.append(p1)\n",
    "        parents.append(p2)\n",
    "\n",
    "    #Reproduction\n",
    "    offspring = []\n",
    "    while len(offspring) < int(N/2):\n",
    "        par = random.sample(parents, 2)\n",
    "        #Crossover + Mutation\n",
    "        if cr >= random.uniform(0,1): #Crossover\n",
    "            p1 = par[0][0]\n",
    "            p2 = par[1][0]\n",
    "            c1, c2 = crossover(p1, p2)\n",
    "\n",
    "            #Mutation\n",
    "            if mr >= random.uniform(0,1):\n",
    "                mutation(c1)\n",
    "\n",
    "            if mr >= random.uniform(0,1):\n",
    "                mutation(c2)\n",
    "\n",
    "\n",
    "            #Decoding the network\n",
    "            network1 = decoding(c1)\n",
    "            network2 = decoding(c2)\n",
    "\n",
    "            #Creating the CNN\n",
    "            cnn1 = CNN(c1, network1[0], network1[1], network1[2])\n",
    "            cnn2 = CNN(c2, network2[0], network2[1], network2[2])\n",
    "\n",
    "            #Evaluate individuals\n",
    "            MAPE,R2, params, mse,f=  training(device, cnn1, num_epochs, loss_func, train_dl, test_dl, lr, w, max_params)\n",
    "            offspring.append([e1,MAPE, params, mse])\n",
    "\n",
    "            MAPE,R2, params, mse,f=  training(device, cnn1, num_epochs, loss_func, train_dl, test_dl, lr, w, max_params)\n",
    "            offspring.append([e1,MAPE, params, mse])\n",
    "\n",
    "    #Replacement with elitism\n",
    "    pop = pop + offspring\n",
    "    pop.sort(key = lambda x: x[1])\n",
    "    pop = pop[:N]\n",
    "\n",
    "    leader = min(pop, key = lambda x: x[1])\n",
    "    bestMSE.append(leader[1])\n",
    "    mape.append(leader[2])\n",
    "    bestParams.append(leader[3])\n",
    "\n",
    "    print('Best MSE: ', leader[3])\n",
    "    print('Best MAPE: ', leader[1])\n",
    "    print('Best No. of Params: ', leader[2])\n",
    "    print('No. of Conv. Layers: ', leader[0].n_conv)\n",
    "    print('No. of FC Layers: ', leader[0].n_full)\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d1c6b",
   "metadata": {},
   "source": [
    "## Guardamos los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "356a41a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time:  0.03194617652777298\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(list(zip(bestMSE,mape, bestParams)), columns = ['MSE', 'MAPE', 'No. Params'])\n",
    "final_networks = []\n",
    "final_connections = []\n",
    "objects = []\n",
    "for member in pop:\n",
    "    p = member[0]\n",
    "    objects.append(p)\n",
    "    n_conv = p.n_conv\n",
    "    n_full = p.n_full\n",
    "    description = 'The network has ' + str(n_conv) + ' convolutional layers ' + 'with: '\n",
    "    for i in range(n_conv):\n",
    "        nfilters = str(p.first_level[i]['nfilters'])\n",
    "        fsize = str(p.first_level[i]['fsize'])\n",
    "        pool = str(p.first_level[i]['pool'])\n",
    "        psize = str(p.first_level[i]['psize'])\n",
    "        layer = '(' + nfilters + ', ' + fsize + ', ' + pool + ', ' + psize + ') '\n",
    "        description += layer\n",
    "    description += 'and '\n",
    "    description += str(n_full)\n",
    "    description += ' '\n",
    "    description += 'fully-connected layers with: '\n",
    "    for i in range(n_conv, n_conv+n_full):\n",
    "        neurons = str(p.first_level[i]['neurons'])\n",
    "        layer = '(' + neurons + ')'\n",
    "        description += layer\n",
    "    description += ' neurons'\n",
    "    final_networks.append(description)\n",
    "    \n",
    "    connections = ''\n",
    "    for bit in p.second_level:\n",
    "        if bit == 1:\n",
    "            connections += 'one - '\n",
    "        if bit == 0:\n",
    "            connections += 'zero - '\n",
    "    final_connections.append(connections)\n",
    "\n",
    "     \n",
    "final_population = pd.DataFrame(list(zip(final_networks, final_connections)), columns = ['Network Architecture', 'Connections'])\n",
    "'''Saving Results as CSV'''\n",
    "final_population.to_csv('arquithecture.csv', index = False)\n",
    "results.to_csv('results.csv', index = False)\n",
    "stop = timeit.default_timer()\n",
    "execution_time = (stop-start)/3600\n",
    "print(\"Execution time: \", execution_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
